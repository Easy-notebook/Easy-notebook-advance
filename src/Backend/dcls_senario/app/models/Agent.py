"""
æµå¼AgentåŸºç±»
æ”¯æŒæ¨¡æ¿åŒ–çš„æµå¼è¾“å‡º
"""
import time
import pandas as pd
from typing import Dict, Any, Generator, List
from abc import ABC, abstractmethod
from app.core.template_parser import template_engine

class Agent(ABC):
    """æµå¼AgentåŸºç±»"""
    
    def __init__(self, 
                 agent_name: str,
                 template_name: str = None,
                 stream: bool = True):
        self.agent_name = agent_name
        self.template_name = template_name
        self.stream = stream
        self.template_engine = template_engine
        self.context = {}
        self.state_updates = {}
        
    def set_context(self, context: Dict[str, Any]):
        """è®¾ç½®æ¨¡æ¿ä¸Šä¸‹æ–‡"""
        self.context.update(context)
    
    def add_context(self, key: str, value: Any):
        """æ·»åŠ å•ä¸ªä¸Šä¸‹æ–‡å˜é‡"""
        self.context[key] = value
    
    @abstractmethod
    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®å¹¶è¿”å›ç»“æœ"""
        pass
    
    def generate_streaming_response(self) -> Generator[Dict[str, Any], None, None]:
        """ç”Ÿæˆæµå¼å“åº”"""
        if not self.template_name:
            # å¦‚æœæ²¡æœ‰æ¨¡æ¿ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            yield from self._generate_traditional_response()
            return
        
        try:
            # ä½¿ç”¨æ¨¡æ¿å¼•æ“ç”Ÿæˆæµå¼å“åº”
            for action in self.template_engine.render_template(self.template_name, self.context):
                # æ·»åŠ agentä¿¡æ¯
                action["agentName"] = self.agent_name
                action["timestamp"] = time.time()
                
                # å¤„ç†çŠ¶æ€æ›´æ–°
                if action.get("action") == "update_state" and "state" in action:
                    self.state_updates.update(action["state"])
                    action["state"] = self.state_updates.copy()
                
                yield action
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡ºçš„å»¶è¿Ÿ
                if self.stream:
                    time.sleep(0.1)
                    
        except Exception as e:
            # é”™è¯¯å¤„ç†
            yield {
                "action": "add",
                "content": f"âŒ Agentæ‰§è¡Œå‡ºé”™: {str(e)}",
                "agentName": self.agent_name,
                "error": True
            }
    
    def _generate_traditional_response(self) -> Generator[Dict[str, Any], None, None]:
        """ä¼ ç»Ÿæ–¹å¼ç”Ÿæˆå“åº”ï¼ˆå…¼å®¹æ€§ï¼‰"""
        yield {
            "action": "is_thinking",
            "content": f"ğŸ¤– {self.agent_name} æ­£åœ¨åˆ†æ...",
            "agentName": self.agent_name
        }
        
        # æ‰§è¡Œåˆ†æ
        result = self.analyze(self.context)
        
        yield {
            "action": "thinking", 
            "content": f"{self.agent_name} åˆ†æè¿‡ç¨‹: {result.get('thinking', 'æ­£åœ¨å¤„ç†æ•°æ®...')}",
            "agentName": self.agent_name
        }
        
        yield {
            "action": "add",
            "content": result.get("content", "åˆ†æå®Œæˆ"),
            "agentName": self.agent_name
        }
        
        if result.get("state"):
            yield {
                "action": "update_state",
                "state": result["state"],
                "agentName": self.agent_name
            }

class DataAnalysisAgent(Agent):
    """æ•°æ®åˆ†æAgent"""
    
    def __init__(self, stream: bool = True):
        super().__init__(
            agent_name="Data Analysis Agent",
            template_name="data_analysis",
            stream=stream
        )
    
    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®é›†"""
        csv_path = data.get("csv_file_path")
        if not csv_path:
            return {"error": "CSVæ–‡ä»¶è·¯å¾„æœªæä¾›"}
        
        try:
            df = pd.read_csv(csv_path)
            
            # æ‰§è¡Œæ•°æ®åˆ†æ
            analysis_result = self._perform_data_analysis(df)
            
            # è®¾ç½®æ¨¡æ¿ä¸Šä¸‹æ–‡
            self.set_context(analysis_result)
            
            return {"success": True, "analysis": analysis_result}
            
        except Exception as e:
            return {"error": f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}"}
    
    def _perform_data_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¦ç»†çš„æ•°æ®åˆ†æ"""
        # åŸºç¡€ä¿¡æ¯
        dataset_shape = f"{df.shape[0]:,} rows Ã— {df.shape[1]} columns"
        feature_count = df.shape[1]
        target_variable = "SalePrice" if "SalePrice" in df.columns else "Unknown"
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_analysis = self._analyze_missing_values(df)
        
        # ç‰¹å¾åˆ†æ
        feature_analysis = self._analyze_features(df)
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_insights(df, missing_analysis, feature_analysis)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(df, missing_analysis)
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = self._calculate_quality_score(missing_analysis, feature_analysis)
        
        return {
            "dataset_shape": dataset_shape,
            "feature_count": feature_count,
            "target_variable": target_variable,
            "missing_values": missing_analysis,
            "feature_analysis": feature_analysis,
            "insights": insights,
            "recommendations": recommendations,
            "quality_score": quality_score,
            "next_steps": ["data_cleaning", "feature_engineering", "model_selection"]
        }
    
    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç¼ºå¤±å€¼"""
        missing_counts = df.isnull().sum()
        total_missing = missing_counts.sum()
        
        if total_missing == 0:
            return {"total": 0, "columns": []}
        
        missing_columns = []
        for col in missing_counts[missing_counts > 0].index:
            missing_columns.append({
                "name": col,
                "count": int(missing_counts[col]),
                "percentage": f"{(missing_counts[col] / len(df) * 100):.1f}"
            })
        
        return {
            "total": int(total_missing),
            "columns": missing_columns
        }
    
    def _analyze_features(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """åˆ†æç‰¹å¾"""
        features = []
        
        for col in df.columns[:10]:  # åªåˆ†æå‰10ä¸ªç‰¹å¾
            feature_info = {
                "name": col,
                "type": str(df[col].dtype),
                "unique_count": int(df[col].nunique()),
                "null_count": int(df[col].isnull().sum()),
                "quality_score": self._calculate_feature_quality(df[col])
            }
            features.append(feature_info)
        
        return features
    
    def _calculate_feature_quality(self, series: pd.Series) -> int:
        """è®¡ç®—ç‰¹å¾è´¨é‡åˆ†æ•°"""
        score = 100
        
        # ç¼ºå¤±å€¼æƒ©ç½š
        null_ratio = series.isnull().sum() / len(series)
        if null_ratio > 0.5:
            score -= 40
        elif null_ratio > 0.2:
            score -= 20
        
        # å”¯ä¸€å€¼æ£€æŸ¥
        unique_ratio = series.nunique() / len(series)
        if unique_ratio == 1:  # æ— å˜åŒ–
            score = 0
        elif unique_ratio > 0.95:  # å‡ ä¹å…¨å”¯ä¸€
            score -= 30
        
        return max(0, score)
    
    def _generate_insights(self, df: pd.DataFrame, missing_analysis: Dict, feature_analysis: List) -> List[str]:
        """ç”Ÿæˆæ•°æ®æ´å¯Ÿ"""
        insights = []
        
        # æ•°æ®è§„æ¨¡æ´å¯Ÿ
        if df.shape[0] > 10000:
            insights.append("ğŸ“Š å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€‚åˆä½¿ç”¨å¤æ‚æ¨¡å‹å’Œé›†æˆæ–¹æ³•")
        elif df.shape[0] < 1000:
            insights.append("âš ï¸ å°è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦æ³¨æ„è¿‡æ‹Ÿåˆé£é™©")
        
        # ç¼ºå¤±å€¼æ´å¯Ÿ
        if missing_analysis["total"] > 0:
            high_missing_cols = [col for col in missing_analysis["columns"] if float(col["percentage"]) > 20]
            if high_missing_cols:
                insights.append(f"ğŸ” {len(high_missing_cols)} ä¸ªç‰¹å¾ç¼ºå¤±å€¼è¶…è¿‡20%ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†")
        
        # ç‰¹å¾è´¨é‡æ´å¯Ÿ
        low_quality_features = [f for f in feature_analysis if f["quality_score"] < 50]
        if low_quality_features:
            insights.append(f"âš ï¸ {len(low_quality_features)} ä¸ªç‰¹å¾è´¨é‡è¾ƒä½ï¼Œå»ºè®®è€ƒè™‘ç§»é™¤æˆ–è½¬æ¢")
        
        # ç›®æ ‡å˜é‡æ´å¯Ÿ
        if "SalePrice" in df.columns:
            price_range = df["SalePrice"].max() - df["SalePrice"].min()
            insights.append(f"ğŸ¯ æˆ¿ä»·èŒƒå›´: ${df['SalePrice'].min():,.0f} - ${df['SalePrice'].max():,.0f}")
        
        return insights
    
    def _generate_recommendations(self, df: pd.DataFrame, missing_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        recommendations = []
        
        # ç¼ºå¤±å€¼å¤„ç†å»ºè®®
        if missing_analysis["total"] > 0:
            recommendations.append("ğŸ”§ å®æ–½ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ï¼šä¸­ä½æ•°/ä¼—æ•°å¡«å……æˆ–åˆ é™¤é«˜ç¼ºå¤±åˆ—")
        
        # ç‰¹å¾å·¥ç¨‹å»ºè®®
        if "SalePrice" in df.columns and df["SalePrice"].skew() > 1:
            recommendations.append("ğŸ“Š å¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¯¹æ•°å˜æ¢ä»¥å‡å°‘åæ–œ")
        
        # æ¨¡å‹é€‰æ‹©å»ºè®®
        if df.shape[1] > 50:
            recommendations.append("ğŸ¤– ä½¿ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯å‡å°‘ç»´åº¦")
        
        recommendations.append("âœ… è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æä»¥å‘ç°æ›´å¤šæ¨¡å¼")
        recommendations.append("ğŸš€ å‡†å¤‡æ•°æ®åˆ†å‰²å’Œäº¤å‰éªŒè¯ç­–ç•¥")
        
        return recommendations
    
    def _calculate_quality_score(self, missing_analysis: Dict, feature_analysis: List) -> int:
        """è®¡ç®—æ•´ä½“æ•°æ®è´¨é‡åˆ†æ•°"""
        score = 100
        
        # ç¼ºå¤±å€¼å½±å“
        if missing_analysis["total"] > 0:
            missing_ratio = missing_analysis["total"] / (len(feature_analysis) * 1000)  # å‡è®¾1000è¡Œ
            score -= min(30, missing_ratio * 100)
        
        # ç‰¹å¾è´¨é‡å½±å“
        if feature_analysis:
            avg_feature_quality = sum(f["quality_score"] for f in feature_analysis) / len(feature_analysis)
            score = int(score * (avg_feature_quality / 100))
        
        return max(0, min(100, score))

class MethodologyAgent(Agent):
    """æ–¹æ³•è®ºè®¾è®¡Agent"""
    
    def __init__(self, stream: bool = True):
        super().__init__(
            agent_name="Methodology Design Agent",
            template_name="methodology_design", 
            stream=stream
        )
    
    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¾è®¡æœºå™¨å­¦ä¹ æ–¹æ³•è®º"""
        # è·å–æ•°æ®åˆ†æç»“æœ
        data_analysis = data.get("data_analysis_results", {})
        dataset_shape = data.get("dataset_shape", (0, 0))
        
        # è®¾è®¡æ–¹æ³•è®º
        methodology = self._design_methodology(data_analysis, dataset_shape)
        
        # è®¾ç½®æ¨¡æ¿ä¸Šä¸‹æ–‡
        self.set_context(methodology)
        
        return {"success": True, "methodology": methodology}
    
    def _design_methodology(self, data_analysis: Dict, shape: tuple) -> Dict[str, Any]:
        """è®¾è®¡å®Œæ•´çš„MLæ–¹æ³•è®º"""
        # åˆ†æé—®é¢˜å¤æ‚åº¦
        complexity_analysis = self._analyze_complexity(data_analysis, shape)
        
        # é€‰æ‹©ç®—æ³•
        algorithms = self._select_algorithms(complexity_analysis)
        
        # è®¾è®¡ç‰¹å¾å·¥ç¨‹
        feature_engineering = self._design_feature_engineering(data_analysis)
        
        # è®¾è®¡éªŒè¯ç­–ç•¥
        validation = self._design_validation_strategy(shape)
        
        return {
            "problem_type": "Regression",
            "complexity_level": complexity_analysis["level"],
            "data_size": complexity_analysis["size"],
            "primary_algorithms": algorithms["primary"],
            "ensemble_methods": algorithms["ensemble"],
            "preprocessing_steps": feature_engineering["preprocessing"],
            "feature_engineering": feature_engineering["creation"],
            "validation_method": validation["method"],
            "cv_folds": validation["folds"],
            "evaluation_metrics": validation["metrics"],
            "target_r2": 0.85,
            "target_rmse": 30000,
            "target_mae": 20000,
            "selected_algorithms": [alg["name"] for alg in algorithms["primary"]],
        }
    
    def _analyze_complexity(self, data_analysis: Dict, shape: tuple) -> Dict[str, Any]:
        """åˆ†æé—®é¢˜å¤æ‚åº¦"""
        complexity = "medium"
        size = "medium"
        
        # åŸºäºæ•°æ®è§„æ¨¡åˆ¤æ–­
        if shape[0] > 5000:
            size = "large"
        elif shape[0] < 1000:
            size = "small"
        
        # åŸºäºç‰¹å¾æ•°é‡å’Œè´¨é‡åˆ¤æ–­å¤æ‚åº¦
        missing_total = data_analysis.get("missing_values", {}).get("total_missing", 0)
        if missing_total > shape[0] * 0.1 or shape[1] > 100:
            complexity = "high"
        elif missing_total == 0 and shape[1] < 20:
            complexity = "low"
        
        return {"level": complexity, "size": size}
    
    def _select_algorithms(self, complexity: Dict) -> Dict[str, List]:
        """é€‰æ‹©åˆé€‚çš„ç®—æ³•"""
        primary = [
            {"name": "Linear Regression", "description": "åŸºç¡€çº¿æ€§æ¨¡å‹ï¼Œé«˜è§£é‡Šæ€§", "use_case": "åŸºçº¿æ¨¡å‹", "expected_performance": "ä¸­ç­‰"},
            {"name": "Random Forest", "description": "é›†æˆå†³ç­–æ ‘ï¼Œé²æ£’æ€§å¼º", "use_case": "ç‰¹å¾é‡è¦æ€§åˆ†æ", "expected_performance": "è‰¯å¥½"},
            {"name": "Gradient Boosting", "description": "åºåˆ—é›†æˆï¼Œé«˜ç²¾åº¦", "use_case": "æ€§èƒ½ä¼˜åŒ–", "expected_performance": "ä¼˜ç§€"}
        ]
        
        ensemble = []
        if complexity["level"] in ["medium", "high"]:
            ensemble = [
                {"name": "Voting Regressor", "description": "å¤šæ¨¡å‹æŠ•ç¥¨é›†æˆ"},
                {"name": "Stacking Regressor", "description": "å…ƒå­¦ä¹ é›†æˆæ–¹æ³•"}
            ]
        
        return {"primary": primary, "ensemble": ensemble}
    
    def _design_feature_engineering(self, data_analysis: Dict) -> Dict[str, List]:
        """è®¾è®¡ç‰¹å¾å·¥ç¨‹ç­–ç•¥"""
        preprocessing = []
        creation = []
        
        # åŸºäºç¼ºå¤±å€¼æƒ…å†µè®¾è®¡é¢„å¤„ç†
        missing_info = data_analysis.get("missing_values", {})
        if missing_info.get("total_missing", 0) > 0:
            preprocessing.append({
                "name": "Missing Value Imputation",
                "description": "å¤„ç†ç¼ºå¤±å€¼",
                "implementation": "ä½¿ç”¨ä¸­ä½æ•°/ä¼—æ•°å¡«å……æˆ–KNNæ’å€¼"
            })
        
        # åŸºäºç›®æ ‡å˜é‡ç‰¹å¾è®¾è®¡è½¬æ¢
        target_info = data_analysis.get("target_analysis", {})
        if target_info.get("skewness", 0) > 1:
            preprocessing.append({
                "name": "Target Transformation", 
                "description": "ç›®æ ‡å˜é‡å¯¹æ•°å˜æ¢",
                "implementation": "np.log1p(y)"
            })
        
        # ç‰¹å¾åˆ›å»ºç­–ç•¥
        creation = [
            "å¤šé¡¹å¼ç‰¹å¾ç”Ÿæˆ",
            "ç‰¹å¾äº¤äº’é¡¹åˆ›å»º", 
            "æ•°å€¼ç‰¹å¾åˆ†ç®±",
            "ç±»åˆ«ç‰¹å¾ç¼–ç "
        ]
        
        return {"preprocessing": preprocessing, "creation": creation}
    
    def _design_validation_strategy(self, shape: tuple) -> Dict[str, Any]:
        """è®¾è®¡éªŒè¯ç­–ç•¥"""
        if shape[0] < 1000:
            return {
                "method": "K-Fold Cross Validation",
                "folds": 3,
                "metrics": ["RÂ²", "RMSE", "MAE"]
            }
        else:
            return {
                "method": "K-Fold Cross Validation", 
                "folds": 5,
                "metrics": ["RÂ²", "RMSE", "MAE", "MAPE"]
            }

# å¯¼å‡ºä¸»è¦ç±»
__all__ = ["Agent", "DataAnalysisAgent", "MethodologyAgent"]
